import os
import cv2
import pandas as pd
from pathlib import Path
from tqdm import tqdm
import hashlib

# ===== Parameters =====
DATASET_PATH = "dog_breed/train"         
LABELS_PATH = "dog_breed/labels.csv"     
OUTPUT_PATH = "dog_breed_cleaned"        
IMG_SIZE = 224                            
MIN_DIM = 50                              
DEDUP = True                              

# ===== Load labels =====
labels_df = pd.read_csv(LABELS_PATH)

# ===== Create breed folders =====
for breed in labels_df['breed'].unique():
    os.makedirs(os.path.join(OUTPUT_PATH, breed), exist_ok=True)

# ===== Helpers =====
seen_hashes = set()  # For duplicate removal

def is_valid_image(img_path):
    img = cv2.imread(str(img_path))
    if img is None:
        return False
    h, w, _ = img.shape
    return h >= MIN_DIM and w >= MIN_DIM

def image_hash(img):
    # MD5 hash of JPEG bytes for exact duplicates
    _, enc = cv2.imencode(".jpg", img)
    return hashlib.md5(enc.tobytes()).hexdigest()

# ===== Process images =====
for _, row in tqdm(labels_df.iterrows(), total=len(labels_df), desc="Cleaning images"):
    img_id, breed = row['id'], row['breed']
    img_path = os.path.join(DATASET_PATH, f"{img_id}.jpg")

    # Skip missing or invalid images
    if not os.path.exists(img_path) or not is_valid_image(img_path):
        continue

    img = cv2.imread(img_path)

    # Resize carefully for CNN
    img = cv2.resize(img, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_CUBIC)

    # Remove duplicates
    if DEDUP:
        hsh = image_hash(img)
        if hsh in seen_hashes:
            continue
        seen_hashes.add(hsh)

    # Save to breed folder
    out_path = os.path.join(OUTPUT_PATH, breed, f"{img_id}.jpg")
    cv2.imwrite(out_path, img)

print("âœ… Dataset cleaned, duplicates removed, resized, and organized by breed!")
